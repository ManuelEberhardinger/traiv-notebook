{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Traiv\n",
    "\n",
    "This notebook contains code for training and testing of different deep learning architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance dataset \n",
    "Balance the dataset as some classes have only a few images. Code retrieved from: https://medium.com/analytics-vidhya/how-to-apply-data-augmentation-to-deal-with-unbalanced-datasets-in-20-lines-of-code-ada8521320c9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.keras import balanced_batch_generator\n",
    "\n",
    "class BalancedDataGenerator(Sequence):\n",
    "    \"\"\"ImageDataGenerator + RandomOversampling\"\"\"\n",
    "    def __init__(self, x, y, datagen, batch_size=32):\n",
    "        self.datagen = datagen\n",
    "        self.batch_size = batch_size\n",
    "        self._shape = x.shape        \n",
    "        datagen.fit(x)\n",
    "        self.gen, self.steps_per_epoch = balanced_batch_generator(x.reshape(x.shape[0], -1), y, sampler=RandomOverSampler(), batch_size=self.batch_size, keep_sparse=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._shape[0] // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_batch, y_batch = self.gen.__next__()\n",
    "        x_batch = x_batch.reshape(-1, *self._shape[1:])\n",
    "        return self.datagen.flow(x_batch, y_batch, batch_size=self.batch_size).next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store data in h5 file for Google Colab\n",
    "\n",
    "Reading information from Google Drive is very slow and the performance benefits of using a GPU on Google Colab are not visible. For more info: https://medium.com/@oribarel/getting-the-most-out-of-your-google-colab-2b0585f82403\n",
    "\n",
    "### Read all images into in memory array with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_mapping(classes):\n",
    "    classes.sort()\n",
    "    class_to_idx = {i: classes[i] for i in range(len(classes))}\n",
    "    return class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Beach', 1: 'Desert', 2: 'Forest', 3: 'Galaxy', 4: 'Glacier', 5: 'Jungle', 6: 'Mountains', 7: 'Ruins', 8: 'Waterfalls'}\n",
      "found 4303 images for Beach\n",
      "found 1313 images for Desert\n",
      "found 4373 images for Forest\n",
      "found 526 images for Galaxy\n",
      "found 970 images for Glacier\n",
      "found 564 images for Jungle\n",
      "found 7028 images for Mountains\n",
      "found 478 images for Ruins\n",
      "found 1209 images for Waterfalls\n",
      "stored 20759 images and 20759 labels in array\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "ROOT = 'dataset/Trainingsset/'\n",
    "IMG_SIZE = (256, 256)\n",
    "\n",
    "images, labels = [], []\n",
    "\n",
    "# loop over directory classes to get all images\n",
    "classes = [x for x in os.listdir(ROOT) if \".\" not in x]\n",
    "class_mappings = get_class_mapping(classes)\n",
    "print(class_mappings)\n",
    "\n",
    "\n",
    "for c, v in class_mappings.items():\n",
    "    c_path = os.path.join(ROOT, v)\n",
    "    c_images = [os.path.join(c_path, x) for x in os.listdir(c_path) if not x.startswith('.')]\n",
    "    print(\"found {} images for {}\".format(len(c_images), v))\n",
    "    for image_path in c_images:\n",
    "        img = Image.open(image_path)\n",
    "        \n",
    "        # skip all grayscale images\n",
    "        if img.mode != 'RGB':\n",
    "            continue\n",
    "            \n",
    "        img = img.resize(IMG_SIZE)\n",
    "        images.append(np.asarray(img))\n",
    "        labels.append(c)\n",
    "        \n",
    "print(\"stored {} images and {} labels in array\".format(len(images), len(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 28 images for 0\n",
      "found 27 images for 1\n",
      "found 23 images for 2\n",
      "found 26 images for 3\n",
      "found 28 images for 4\n",
      "found 21 images for 5\n",
      "found 29 images for 6\n",
      "found 28 images for 7\n",
      "found 26 images for 8\n",
      "stored 236 images and 236 labels in array\n"
     ]
    }
   ],
   "source": [
    "ROOT = 'dataset/Testset/'\n",
    "IMG_SIZE = (256, 256)\n",
    "\n",
    "images_test, labels_test = [], []\n",
    "\n",
    "for c, v in class_mappings.items():\n",
    "    c_path = os.path.join(ROOT, v)\n",
    "    c_images = [os.path.join(c_path, x) for x in os.listdir(c_path) if not x.startswith('.')]\n",
    "    print(\"found {} images for {}\".format(len(c_images), c))\n",
    "    for image_path in c_images:\n",
    "        img = Image.open(image_path)\n",
    "        if img.mode == 'L':\n",
    "            continue\n",
    "            \n",
    "        img = img.resize(IMG_SIZE)\n",
    "        images_test.append(np.asarray(img))\n",
    "        labels_test.append(c)\n",
    "        \n",
    "print(\"stored {} images and {} labels in array\".format(len(images_test), len(labels_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store images and labels in h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'data.h5'\n",
    "\n",
    "with h5py.File(fileName, \"w\") as out:\n",
    "    out.create_dataset(\"X_train\", np.shape(images), dtype='u1', data=np.asarray(images))\n",
    "    out.create_dataset(\"Y_train\", np.shape(labels), dtype='u1',  data=np.asarray(labels))    \n",
    "    out.create_dataset(\"X_test\", np.shape(images_test), dtype='u1', data=np.asarray(images_test))\n",
    "    out.create_dataset(\"Y_test\", np.shape(labels_test), dtype='u1', data=np.asarray(labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing and data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20764 images belonging to 9 classes.\n",
      "Found 0 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_data_dir = 'dataset/Trainingsset/'\n",
    "img_size = (224,224)\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "        #validation_split=0.2)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training') # set as training data\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    train_data_dir, # same directory as training data\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation') # set as validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1.6332791075993494,\n",
       " 1: 5.352627570449353,\n",
       " 2: 1.607134690144066,\n",
       " 3: 13.361216730038024,\n",
       " 4: 7.245360824742268,\n",
       " 5: 12.460992907801419,\n",
       " 6: 1.0,\n",
       " 7: 14.702928870292887,\n",
       " 8: 5.813068651778329}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get class weights\n",
    "counter = Counter(train_generator.classes)                          \n",
    "max_val = float(max(counter.values()))       \n",
    "class_weights = {class_id : max_val/num_images for class_id, num_images in counter.items()}        \n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 519 steps, validate for 129 steps\n",
      "Epoch 1/5\n",
      " 64/519 [==>...........................] - ETA: 8:06 - loss: 8.1889 - accuracy: 0.1758"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manuel/opt/anaconda3/envs/data-science/lib/python3.7/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "519/519 [==============================] - 622s 1s/step - loss: 5.3725 - accuracy: 0.3058 - val_loss: 4.5035 - val_accuracy: 0.5942\n",
      "Epoch 2/5\n",
      "519/519 [==============================] - 616s 1s/step - loss: 4.2141 - accuracy: 0.4001 - val_loss: 5.6441 - val_accuracy: 0.4542\n",
      "Epoch 3/5\n",
      "519/519 [==============================] - 599s 1s/step - loss: 4.0138 - accuracy: 0.4137 - val_loss: 4.7662 - val_accuracy: 0.5397\n",
      "Epoch 4/5\n",
      "519/519 [==============================] - 563s 1s/step - loss: 3.8453 - accuracy: 0.4300 - val_loss: 6.1935 - val_accuracy: 0.5579\n",
      "Epoch 5/5\n",
      "519/519 [==============================] - 562s 1s/step - loss: 3.8264 - accuracy: 0.4347 - val_loss: 5.3864 - val_accuracy: 0.4608\n",
      "0 input_3\n",
      "1 conv2d_188\n",
      "2 batch_normalization_188\n",
      "3 activation_188\n",
      "4 conv2d_189\n",
      "5 batch_normalization_189\n",
      "6 activation_189\n",
      "7 conv2d_190\n",
      "8 batch_normalization_190\n",
      "9 activation_190\n",
      "10 max_pooling2d_8\n",
      "11 conv2d_191\n",
      "12 batch_normalization_191\n",
      "13 activation_191\n",
      "14 conv2d_192\n",
      "15 batch_normalization_192\n",
      "16 activation_192\n",
      "17 max_pooling2d_9\n",
      "18 conv2d_196\n",
      "19 batch_normalization_196\n",
      "20 activation_196\n",
      "21 conv2d_194\n",
      "22 conv2d_197\n",
      "23 batch_normalization_194\n",
      "24 batch_normalization_197\n",
      "25 activation_194\n",
      "26 activation_197\n",
      "27 average_pooling2d_18\n",
      "28 conv2d_193\n",
      "29 conv2d_195\n",
      "30 conv2d_198\n",
      "31 conv2d_199\n",
      "32 batch_normalization_193\n",
      "33 batch_normalization_195\n",
      "34 batch_normalization_198\n",
      "35 batch_normalization_199\n",
      "36 activation_193\n",
      "37 activation_195\n",
      "38 activation_198\n",
      "39 activation_199\n",
      "40 mixed0\n",
      "41 conv2d_203\n",
      "42 batch_normalization_203\n",
      "43 activation_203\n",
      "44 conv2d_201\n",
      "45 conv2d_204\n",
      "46 batch_normalization_201\n",
      "47 batch_normalization_204\n",
      "48 activation_201\n",
      "49 activation_204\n",
      "50 average_pooling2d_19\n",
      "51 conv2d_200\n",
      "52 conv2d_202\n",
      "53 conv2d_205\n",
      "54 conv2d_206\n",
      "55 batch_normalization_200\n",
      "56 batch_normalization_202\n",
      "57 batch_normalization_205\n",
      "58 batch_normalization_206\n",
      "59 activation_200\n",
      "60 activation_202\n",
      "61 activation_205\n",
      "62 activation_206\n",
      "63 mixed1\n",
      "64 conv2d_210\n",
      "65 batch_normalization_210\n",
      "66 activation_210\n",
      "67 conv2d_208\n",
      "68 conv2d_211\n",
      "69 batch_normalization_208\n",
      "70 batch_normalization_211\n",
      "71 activation_208\n",
      "72 activation_211\n",
      "73 average_pooling2d_20\n",
      "74 conv2d_207\n",
      "75 conv2d_209\n",
      "76 conv2d_212\n",
      "77 conv2d_213\n",
      "78 batch_normalization_207\n",
      "79 batch_normalization_209\n",
      "80 batch_normalization_212\n",
      "81 batch_normalization_213\n",
      "82 activation_207\n",
      "83 activation_209\n",
      "84 activation_212\n",
      "85 activation_213\n",
      "86 mixed2\n",
      "87 conv2d_215\n",
      "88 batch_normalization_215\n",
      "89 activation_215\n",
      "90 conv2d_216\n",
      "91 batch_normalization_216\n",
      "92 activation_216\n",
      "93 conv2d_214\n",
      "94 conv2d_217\n",
      "95 batch_normalization_214\n",
      "96 batch_normalization_217\n",
      "97 activation_214\n",
      "98 activation_217\n",
      "99 max_pooling2d_10\n",
      "100 mixed3\n",
      "101 conv2d_222\n",
      "102 batch_normalization_222\n",
      "103 activation_222\n",
      "104 conv2d_223\n",
      "105 batch_normalization_223\n",
      "106 activation_223\n",
      "107 conv2d_219\n",
      "108 conv2d_224\n",
      "109 batch_normalization_219\n",
      "110 batch_normalization_224\n",
      "111 activation_219\n",
      "112 activation_224\n",
      "113 conv2d_220\n",
      "114 conv2d_225\n",
      "115 batch_normalization_220\n",
      "116 batch_normalization_225\n",
      "117 activation_220\n",
      "118 activation_225\n",
      "119 average_pooling2d_21\n",
      "120 conv2d_218\n",
      "121 conv2d_221\n",
      "122 conv2d_226\n",
      "123 conv2d_227\n",
      "124 batch_normalization_218\n",
      "125 batch_normalization_221\n",
      "126 batch_normalization_226\n",
      "127 batch_normalization_227\n",
      "128 activation_218\n",
      "129 activation_221\n",
      "130 activation_226\n",
      "131 activation_227\n",
      "132 mixed4\n",
      "133 conv2d_232\n",
      "134 batch_normalization_232\n",
      "135 activation_232\n",
      "136 conv2d_233\n",
      "137 batch_normalization_233\n",
      "138 activation_233\n",
      "139 conv2d_229\n",
      "140 conv2d_234\n",
      "141 batch_normalization_229\n",
      "142 batch_normalization_234\n",
      "143 activation_229\n",
      "144 activation_234\n",
      "145 conv2d_230\n",
      "146 conv2d_235\n",
      "147 batch_normalization_230\n",
      "148 batch_normalization_235\n",
      "149 activation_230\n",
      "150 activation_235\n",
      "151 average_pooling2d_22\n",
      "152 conv2d_228\n",
      "153 conv2d_231\n",
      "154 conv2d_236\n",
      "155 conv2d_237\n",
      "156 batch_normalization_228\n",
      "157 batch_normalization_231\n",
      "158 batch_normalization_236\n",
      "159 batch_normalization_237\n",
      "160 activation_228\n",
      "161 activation_231\n",
      "162 activation_236\n",
      "163 activation_237\n",
      "164 mixed5\n",
      "165 conv2d_242\n",
      "166 batch_normalization_242\n",
      "167 activation_242\n",
      "168 conv2d_243\n",
      "169 batch_normalization_243\n",
      "170 activation_243\n",
      "171 conv2d_239\n",
      "172 conv2d_244\n",
      "173 batch_normalization_239\n",
      "174 batch_normalization_244\n",
      "175 activation_239\n",
      "176 activation_244\n",
      "177 conv2d_240\n",
      "178 conv2d_245\n",
      "179 batch_normalization_240\n",
      "180 batch_normalization_245\n",
      "181 activation_240\n",
      "182 activation_245\n",
      "183 average_pooling2d_23\n",
      "184 conv2d_238\n",
      "185 conv2d_241\n",
      "186 conv2d_246\n",
      "187 conv2d_247\n",
      "188 batch_normalization_238\n",
      "189 batch_normalization_241\n",
      "190 batch_normalization_246\n",
      "191 batch_normalization_247\n",
      "192 activation_238\n",
      "193 activation_241\n",
      "194 activation_246\n",
      "195 activation_247\n",
      "196 mixed6\n",
      "197 conv2d_252\n",
      "198 batch_normalization_252\n",
      "199 activation_252\n",
      "200 conv2d_253\n",
      "201 batch_normalization_253\n",
      "202 activation_253\n",
      "203 conv2d_249\n",
      "204 conv2d_254\n",
      "205 batch_normalization_249\n",
      "206 batch_normalization_254\n",
      "207 activation_249\n",
      "208 activation_254\n",
      "209 conv2d_250\n",
      "210 conv2d_255\n",
      "211 batch_normalization_250\n",
      "212 batch_normalization_255\n",
      "213 activation_250\n",
      "214 activation_255\n",
      "215 average_pooling2d_24\n",
      "216 conv2d_248\n",
      "217 conv2d_251\n",
      "218 conv2d_256\n",
      "219 conv2d_257\n",
      "220 batch_normalization_248\n",
      "221 batch_normalization_251\n",
      "222 batch_normalization_256\n",
      "223 batch_normalization_257\n",
      "224 activation_248\n",
      "225 activation_251\n",
      "226 activation_256\n",
      "227 activation_257\n",
      "228 mixed7\n",
      "229 conv2d_260\n",
      "230 batch_normalization_260\n",
      "231 activation_260\n",
      "232 conv2d_261\n",
      "233 batch_normalization_261\n",
      "234 activation_261\n",
      "235 conv2d_258\n",
      "236 conv2d_262\n",
      "237 batch_normalization_258\n",
      "238 batch_normalization_262\n",
      "239 activation_258\n",
      "240 activation_262\n",
      "241 conv2d_259\n",
      "242 conv2d_263\n",
      "243 batch_normalization_259\n",
      "244 batch_normalization_263\n",
      "245 activation_259\n",
      "246 activation_263\n",
      "247 max_pooling2d_11\n",
      "248 mixed8\n",
      "249 conv2d_268\n",
      "250 batch_normalization_268\n",
      "251 activation_268\n",
      "252 conv2d_265\n",
      "253 conv2d_269\n",
      "254 batch_normalization_265\n",
      "255 batch_normalization_269\n",
      "256 activation_265\n",
      "257 activation_269\n",
      "258 conv2d_266\n",
      "259 conv2d_267\n",
      "260 conv2d_270\n",
      "261 conv2d_271\n",
      "262 average_pooling2d_25\n",
      "263 conv2d_264\n",
      "264 batch_normalization_266\n",
      "265 batch_normalization_267\n",
      "266 batch_normalization_270\n",
      "267 batch_normalization_271\n",
      "268 conv2d_272\n",
      "269 batch_normalization_264\n",
      "270 activation_266\n",
      "271 activation_267\n",
      "272 activation_270\n",
      "273 activation_271\n",
      "274 batch_normalization_272\n",
      "275 activation_264\n",
      "276 mixed9_0\n",
      "277 concatenate_4\n",
      "278 activation_272\n",
      "279 mixed9\n",
      "280 conv2d_277\n",
      "281 batch_normalization_277\n",
      "282 activation_277\n",
      "283 conv2d_274\n",
      "284 conv2d_278\n",
      "285 batch_normalization_274\n",
      "286 batch_normalization_278\n",
      "287 activation_274\n",
      "288 activation_278\n",
      "289 conv2d_275\n",
      "290 conv2d_276\n",
      "291 conv2d_279\n",
      "292 conv2d_280\n",
      "293 average_pooling2d_26\n",
      "294 conv2d_273\n",
      "295 batch_normalization_275\n",
      "296 batch_normalization_276\n",
      "297 batch_normalization_279\n",
      "298 batch_normalization_280\n",
      "299 conv2d_281\n",
      "300 batch_normalization_273\n",
      "301 activation_275\n",
      "302 activation_276\n",
      "303 activation_279\n",
      "304 activation_280\n",
      "305 batch_normalization_281\n",
      "306 activation_273\n",
      "307 mixed9_1\n",
      "308 concatenate_5\n",
      "309 activation_281\n",
      "310 mixed10\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 519 steps, validate for 129 steps\n",
      "Epoch 1/5\n",
      " 61/519 [==>...........................] - ETA: 8:58 - loss: 3.4659 - accuracy: 0.4283"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manuel/opt/anaconda3/envs/data-science/lib/python3.7/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "519/519 [==============================] - 678s 1s/step - loss: 3.2937 - accuracy: 0.4644 - val_loss: 4.9145 - val_accuracy: 0.5952\n",
      "Epoch 2/5\n",
      "519/519 [==============================] - 721s 1s/step - loss: 2.9682 - accuracy: 0.5021 - val_loss: 4.4926 - val_accuracy: 0.6090\n",
      "Epoch 3/5\n",
      "519/519 [==============================] - 727s 1s/step - loss: 2.7669 - accuracy: 0.5235 - val_loss: 4.5219 - val_accuracy: 0.6102\n",
      "Epoch 4/5\n",
      "519/519 [==============================] - 727s 1s/step - loss: 2.6189 - accuracy: 0.5318 - val_loss: 4.6623 - val_accuracy: 0.5906\n",
      "Epoch 5/5\n",
      "519/519 [==============================] - 789s 2s/step - loss: 2.5408 - accuracy: 0.5459 - val_loss: 4.5498 - val_accuracy: 0.5911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc0da5c7810>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from collections import Counter\n",
    "\n",
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(9, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# get class weights\n",
    "counter = Counter(train_generator.classes)                          \n",
    "max_val = float(max(counter.values()))       \n",
    "class_weights = {class_id : max_val/num_images for class_id, num_images in counter.items()}                     \n",
    "\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "step_size_train=train_generator.n//train_generator.batch_size\n",
    "model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=step_size_train,\n",
    "        epochs=5,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_generator.n // batch_size,\n",
    "        class_weight=class_weights)\n",
    "\n",
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    print(i, layer.name)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=step_size_train,\n",
    "        epochs=5,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_generator.n // batch_size,\n",
    "        class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16616"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_generator.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = ... # load your data\n",
    "datagen = ImageDataGenerator()\n",
    "balanced_gen = BalancedDataGenerator(x, y, datagen, batch_size=32)\n",
    "steps_per_epoch = balanced_gen.steps_per_epoch\n",
    "model = ... # define your model\n",
    "model.compile(...) # define your compile parameters\n",
    "model.fit_generator(balanced_gen, steps_per_epoch, ...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
